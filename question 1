import numpy as np

def scaled_dot_product_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray):
    """
    Computes the Scaled Dot-Product Attention.

    Attention(Q, K, V) = softmax((Q * K^T) / sqrt(d_k)) * V

    Args:
        Q (np.ndarray): Query matrix (batch_size, seq_len_q, d_k)
        K (np.ndarray): Key matrix (batch_size, seq_len_k, d_k)
        V (np.ndarray): Value matrix (batch_size, seq_len_v, d_v)
            (Note: seq_len_k must equal seq_len_v)

    Returns:
        tuple: (context_vector, attention_weights)
            - context_vector (np.ndarray): Resulting context vector (batch_size, seq_len_q, d_v)
            - attention_weights (np.ndarray): Attention weights matrix (batch_size, seq_len_q, seq_len_k)
    """

    # 1. Calculate the raw alignment scores (Q * K^T)
    # The transpose is applied to the last two dimensions (d_k and seq_len_k)
    # For a 3D tensor (B, S, D), np.swapaxes swaps the axes S and D for K
    K_T = np.swapaxes(K, -2, -1)
    
    # Raw scores: (B, seq_len_q, d_k) @ (B, d_k, seq_len_k) -> (B, seq_len_q, seq_len_k)
    attention_scores = np.matmul(Q, K_T)

    # 2. Scale the scores
    # d_k is the dimension of the keys (the last dimension of Q and K)
    d_k = Q.shape[-1]
    
    # Scaling factor:
    scaling_factor = np.sqrt(d_k)
    
    # Scaled scores:
    scaled_attention_scores = attention_scores / scaling_factor

    # 3. Apply Softmax to get attention weights
    # Softmax is applied across the last dimension (seq_len_k)
    def softmax(x):
        # Subtract max for numerical stability
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

    attention_weights = softmax(scaled_attention_scores)

    # 4. Compute the context vector (Weights * V)
    # Context vector: (B, seq_len_q, seq_len_k) @ (B, seq_len_k, d_v) -> (B, seq_len_q, d_v)
    context_vector = np.matmul(attention_weights, V)

    return context_vector, attention_weights

# --- Example Usage ---
print("--- Example Usage ---")

# Define dimensions
B = 1  # Batch size
seq_len_q = 4  # Query sequence length (e.g., words being translated)
seq_len_k = 5  # Key/Value sequence length (e.g., words in the source sentence)
d_k = 3  # Key dimension
d_v = 6  # Value dimension

# Create dummy input matrices (using random data for illustration)
# Q: (1, 4, 3)
Q_test = np.random.rand(B, seq_len_q, d_k)
# K: (1, 5, 3)
K_test = np.random.rand(B, seq_len_k, d_k)
# V: (1, 5, 6)
V_test = np.random.rand(B, seq_len_k, d_v)

# Compute attention
context, weights = scaled_dot_product_attention(Q_test, K_test, V_test)

# Print results and shapes
print(f"Q shape: {Q_test.shape}")
print(f"K shape: {K_test.shape}")
print(f"V shape: {V_test.shape}\n")

print(f"Attention Weights shape (seq_len_q x seq_len_k): {weights.shape}")
print(f"Context Vector shape (seq_len_q x d_v): {context.shape}")

print("\nSample Attention Weights (first query):")
# Show weights for the first query token across all keys
print(weights[0, 0, :])
